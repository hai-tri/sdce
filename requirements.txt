# Core dependencies
torch>=2.0.0
transformers>=4.35.0
datasets>=2.14.0
tokenizers>=0.14.0

# Configuration
pyyaml>=6.0
omegaconf>=2.3.0

# Logging and monitoring
wandb>=0.15.0
tqdm>=4.65.0

# Numerical computing
numpy>=1.24.0

# Optional: Flash Attention (requires CUDA)
# flash-attn>=2.3.0

# Optional: Accelerate for multi-GPU training
# accelerate>=0.24.0
