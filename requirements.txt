# Core dependencies
torch>=2.0.0
transformers>=4.35.0
datasets>=2.14.0
tokenizers>=0.14.0
hf_xet>=1.0.0

# Configuration
pyyaml>=6.0
omegaconf>=2.3.0

# Logging and monitoring
wandb>=0.15.0
tqdm>=4.65.0

# Numerical computing
numpy>=1.24.0

# Benchmarking (lm-evaluation-harness)
lm-eval>=0.4.0

# Flash Attention 2 (for 4090/Ampere+ GPUs)
flash-attn>=2.3.0

# Optional: Accelerate for multi-GPU training
# accelerate>=0.24.0
