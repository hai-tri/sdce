# Surrogate-Assisted Language Model Training Configuration
# =========================================================
# This configuration file demonstrates how to set up training
# with a surrogate model providing guidance signals.

# Base model configuration
model:
  name_or_path: "gpt2"  # HuggingFace model name or local path
  dtype: "float16"      # float16, bfloat16, or float32
  use_flash_attention: false
  gradient_checkpointing: false
  trust_remote_code: false

# Surrogate model configuration
surrogate:
  name_or_path: "Qwen/Qwen3-0.6B"  # Surrogate model for guidance
  dtype: "float16"
  k: 6              # Number of top-k tokens to consider from surrogate
  enabled: true     # Set to false to disable surrogate guidance
  trust_remote_code: true
  # Surrogate loss weight scheduler (cosine decay, no warmup)
  # weight = final + (initial - final) * 0.5 * (1 + cos(pi * step / total_steps))
  loss_weight_initial: 1.0  # Starting weight for surrogate loss
  loss_weight_final: 0.0    # Final weight (decays to this via cosine schedule)
  loss_weight_initial: 1.0  # Initial weight for surrogate loss
  loss_weight_final: 0.0    # Final weight after cosine decay (reaches this at end of training)

# Data configuration
data:
  dataset_name: "wikitext"          # HuggingFace dataset name
  dataset_config: "wikitext-2-raw-v1"  # Dataset configuration/subset
  dataset_split: "train"
  eval_split: "validation"
  text_column: "text"               # Column containing text data
  max_seq_length: 1024
  preprocessing_num_workers: 4
  # For custom datasets, use these instead:
  # train_file: "/path/to/train.json"
  # eval_file: "/path/to/eval.json"

# Training configuration
training:
  output_dir: "./outputs"
  num_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 4
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_ratio: 0.1
  # warmup_steps: 100  # Use this instead of warmup_ratio if preferred
  max_grad_norm: 1.0
  lr_scheduler_type: "cosine"  # cosine, linear, constant, etc.
  
  # Mixed precision training
  mixed_precision: "fp16"  # fp16, bf16, or no
  
  # Logging and checkpointing
  logging_steps: 10
  eval_steps: 500
  save_steps: 1000
  save_total_limit: 3
  
  # Auxiliary loss (from PaLM)
  use_z_loss: false
  z_loss_multiplier: 1.0e-4
  
  # Random seed
  seed: 42
  
  # Resume training
  # resume_from_checkpoint: "./outputs/checkpoint-1000"
  
  # Weights & Biases logging
  # wandb_project: "surrogate-lm-training"
  # wandb_run_name: "gpt2-with-qwen-surrogate"
  # wandb_entity: "your-entity"
