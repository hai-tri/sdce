# Surrogate-Assisted Language Model Training Configuration
# =========================================================
# This configuration file demonstrates how to set up training
# with a surrogate model providing guidance signals.

# Base model configuration
model:
  name_or_path: "gpt2"  # HuggingFace model name or local path
  dtype: "float16"      # float16, bfloat16, or float32
  use_flash_attention: false
  gradient_checkpointing: false
  trust_remote_code: false

# Surrogate model configuration
surrogate:
  name_or_path: "Qwen/Qwen3-0.6B"  # Surrogate model for guidance
  dtype: "float16"
  k: 6              # Number of top-k tokens to consider from surrogate
  enabled: true     # Set to false to disable surrogate guidance
  trust_remote_code: true
  # Surrogate loss weight scheduler (cosine decay, no warmup)
  # weight = final + (initial - final) * 0.5 * (1 + cos(pi * step / total_steps))
  loss_weight_initial: 1.0  # Starting weight for surrogate loss
  loss_weight_final: 0.0    # Final weight (decays to this via cosine schedule)

# Data configuration
data:
  dataset_name: "wikitext"          # HuggingFace dataset name
  dataset_config: "wikitext-2-raw-v1"  # Dataset configuration/subset
  dataset_split: "train"
  eval_split: "validation"
  text_column: "text"               # Column containing text data
  max_seq_length: 1024
  preprocessing_num_workers: 4
  # For custom datasets, use these instead:
  # train_file: "/path/to/train.json"
  # eval_file: "/path/to/eval.json"

# Training configuration
training:
  output_dir: "./outputs"
  num_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 4
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_ratio: 0.1
  # warmup_steps: 100  # Use this instead of warmup_ratio if preferred
  max_grad_norm: 1.0
  lr_scheduler_type: "cosine"  # cosine, linear, constant, etc.
  
  # Device selection: "auto", "cuda", "mps", "tpu", "cpu"
  device: "auto"
  
  # TPU-specific options (only used when device: "tpu")
  tpu_cores: 1          # 1 for single core, 8 for TPU v3-8, etc.
  tpu_metrics_debug: false
  
  # Mixed precision training
  # - For GPU: "fp16" or "bf16" 
  # - For TPU: "bf16" recommended (TPU v3+ native), "no" for float32
  mixed_precision: "fp16"
  
  # Logging and checkpointing
  logging_steps: 10
  eval_steps: 500
  save_steps: 1000
  save_total_limit: 3
  
  # Auxiliary loss (from PaLM)
  use_z_loss: false
  z_loss_multiplier: 1.0e-4
  
  # Random seed
  seed: 42
  
  # Resume training
  # resume_from_checkpoint: "./outputs/checkpoint-1000"
  
  # Weights & Biases logging
  wandb_project: "project-name"
  wandb_run_name: "run-name"
  wandb_entity: "entity-name"

# Benchmark evaluation configuration (lm-evaluation-harness)
evaluation:
  enabled: true                    # Enable/disable benchmark evaluation
  eval_interval: 500              # Run benchmarks every N steps
  
  tasks:
    - "hellaswag"
    - "arc_easy"
    - "piqa"
    - "winogrande"
    - "boolq"
    - "mmlu"
    - "gsm8k"
    - "arc_challenge"
    - 'arc_easy'
    - 'lambada_openai'
    - 'bbh'
    - 'agieval'
    - 'openbookqa'
    - 'math'
    
  num_fewshot: 0                   # Number of few-shot examples (0 for zero-shot)
  batch_size: 8                    # Batch size for evaluation
  limit: null                      # Limit examples per task (null for all)
  
  # Logging options
  log_individual_tasks: true       # Log score for each task separately
  log_aggregate_score: true        # Log mean score across all tasks
