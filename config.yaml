# Surrogate-Assisted Language Model Training Configuration
# =========================================================
# Pretraining TinyLlama 1.1B from scratch on OpenWebText
# Optimized for RTX 4090 (24GB VRAM)

# Base model configuration
model:
  name_or_path: "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"  # Architecture source
  dtype: "bfloat16"     # bfloat16 recommended for training stability
  use_flash_attention: true   # Enable Flash Attention 2 for efficiency
  gradient_checkpointing: true  # Enable to save VRAM
  trust_remote_code: false
  
  # IMPORTANT: Initialize weights randomly for pretraining from scratch
  init_from_scratch: true
  
  # Optional: Override model config parameters
  # custom_config:
  #   hidden_size: 2048
  #   num_hidden_layers: 22
  #   num_attention_heads: 32

# Surrogate model configuration
surrogate:
  name_or_path: "Qwen/Qwen3-0.6B"  # Surrogate model for guidance
  dtype: "bfloat16"
  # Probability threshold for token selection (replaces fixed k)
  # Selects all surrogate tokens with probability > prob_threshold
  prob_threshold: 0.03
  # Maximum tokens to consider per position (for memory efficiency)
  max_tokens: 100
  enabled: true     # Set to false to disable surrogate guidance
  trust_remote_code: true
  # Surrogate loss weight scheduler (cosine decay)
  loss_weight_initial: 1.0  # Starting weight for surrogate loss
  loss_weight_final: 0.0    # Final weight (decays to this via cosine schedule)

# Distillation configuration
distillation:
  mode: "surrogate"           # "surrogate", "kd", or "none"
  
  # Knowledge Distillation (KD) specific parameters
  # Only used when mode: "kd"
  kd_temperature: 4.0
  kd_alpha: 0.5
  
  # Alpha scheduling (optional, for "kd" mode)
  kd_alpha_schedule: false
  kd_alpha_initial: 0.9
  kd_alpha_final: 0.1

# Data configuration
data:
  dataset_name: "openwebtext"       # OpenWebText dataset
  dataset_config: null              # No config needed for openwebtext
  dataset_split: "train"
  # OpenWebText has no validation split, so we create one
  eval_split: "validation"          # Will be auto-created
  eval_ratio: 0.005                 # 0.5% of data for evaluation (standard ratio)
  text_column: "text"
  max_seq_length: 2048              # TinyLlama supports 2048 context
  preprocessing_num_workers: 8      # Increase for faster preprocessing

# Training configuration
training:
  output_dir: "./outputs/tinyllama-1.1b-scratch-openwebtext"
  num_epochs: 1                     # For pretraining, often 1-2 epochs over large data
  
  # Batch size tuned for 4090 (24GB) with gradient checkpointing
  # Effective batch size = 4 * 16 = 64
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 16
  
  learning_rate: 3.0e-4             # Higher LR typical for pretraining from scratch
  weight_decay: 0.1                 # Higher weight decay for pretraining
  warmup_ratio: 0.01                # ~1% warmup is common for pretraining
  max_grad_norm: 1.0
  lr_scheduler_type: "cosine"
  
  # Device selection
  device: "auto"                    # Will auto-detect CUDA
  
  # Mixed precision - bf16 recommended for training stability
  mixed_precision: "bf16"
  
  # Logging and checkpointing
  logging_steps: 50
  eval_steps: 2000                  # Evaluate every 2k steps
  save_steps: 5000                  # Save checkpoint every 5k steps
  save_total_limit: 3               # Keep only last 3 checkpoints
  
  # Auxiliary loss (from PaLM) - helps with training stability
  use_z_loss: true
  z_loss_multiplier: 1.0e-4
  
  # Random seed
  seed: 42
  
  # Weights & Biases logging
  wandb_project: "SDCE"
  wandb_run_name: "tinyllama-1.1b-scratch-openwebtext"
  wandb_entity: "nathanngtruong-university-of-california-berkeley"

# Benchmark evaluation configuration
evaluation:
  enabled: true
  eval_interval: 10000              # Run benchmarks every 10k steps
  
  tasks:
    - "hellaswag"
    - "piqa"
    - "winogrande"
    - "boolq"
    - "arc_challenge"
    - "lambada_openai"
    
  num_fewshot: 0
  batch_size: 8
  limit: null
  
  log_individual_tasks: true
  log_aggregate_score: true
